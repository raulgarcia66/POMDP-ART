{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sanity_check_prob (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Pipe\n",
    "\n",
    "\"\"\"\n",
    "Returns rows whose sums are not unity. \n",
    "\"\"\"\n",
    "function sanity_check_prob(P::Matrix{T}; tol::Float64 = 1E-6) where T\n",
    "    return @pipe filter(row_ind -> abs(sum(P[row_ind,:]) - 1.0) > tol, 1:size(P,1)) |> map(bad_row -> (bad_row, sum(P[bad_row,:])), _)\n",
    "end\n",
    "\n",
    "function sanity_check_prob(x::Vector{T}; tol::Float64 = 1E-6) where T\n",
    "    if abs(sum(x) - 1.0) > tol\n",
    "        println(\"Invalid probabilities\")\n",
    "    else\n",
    "        println(\"Valid probabilities\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#### Discount factor\n",
    "# %f\n",
    "discount = 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min/Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"reward\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Min/Max\n",
    "# [ reward, cost ]\n",
    "# values =  \"cost\" # to minimize\n",
    "values = \"reward\" # to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### States\n",
    "# [ %d, <list-of-states> ]\n",
    "# When an integer is given, the states are enumerated starting from 0\n",
    "# Delimeters are white space\n",
    "# Since pomdp-solver requires stationary parameters, we will consider time as a component of the state\n",
    "# Hence each state consists of ΔNTCP, b, and t\n",
    "ΔNTCP_states = 0:12\n",
    "budget = 3\n",
    "horizon = 4  # 4 decision epochs; this is not the pomdp-solve optional parameter\n",
    "\n",
    "# TODO: Do we need horizon+1 states? My intuition says yes, since the observation of the last decision epoch T is observed at T+1\n",
    "\n",
    "states = String[]\n",
    "for t = 1:(horizon+1), b in budget:-1:0, ΔNTCP in ΔNTCP_states\n",
    "    push!(states, \"$(ΔNTCP)_$(b)_$t\")\n",
    "end\n",
    "push!(states, \"Forbidden\")  # add dummy absorbing state for when replanning is not allowed\n",
    "\n",
    "num_ΔNTCP_states = length(ΔNTCP_states)\n",
    "num_budget_states = budget + 1\n",
    "num_states = length(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261-element Vector{String}:\n",
       " \"0_3_1\"\n",
       " \"1_3_1\"\n",
       " \"2_3_1\"\n",
       " \"3_3_1\"\n",
       " \"4_3_1\"\n",
       " \"5_3_1\"\n",
       " \"6_3_1\"\n",
       " \"7_3_1\"\n",
       " \"8_3_1\"\n",
       " \"9_3_1\"\n",
       " ⋮\n",
       " \"5_0_5\"\n",
       " \"6_0_5\"\n",
       " \"7_0_5\"\n",
       " \"8_0_5\"\n",
       " \"9_0_5\"\n",
       " \"10_0_5\"\n",
       " \"11_0_5\"\n",
       " \"12_0_5\"\n",
       " \"Forbidden\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display states\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Actions\n",
    "# [ %d, <list-of-actions> ]\n",
    "# When a list is given, the elements may be referred to by name or by 0-based indexing\n",
    "actions = [\"Replan\", \"Continue\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Observations\n",
    "# [ %d, <list-of-observations> ]\n",
    "# Delimeters are white space\n",
    "# An observation is observed after the state transitions\n",
    "# Will assume observations are ordered from best to worst, with high pain being worse than a high BMI drop\n",
    "observations = String[]\n",
    "levels = [\"Low\", \"Med\", \"High\"]\n",
    "for pain_level in levels, bmi_level in levels\n",
    "    push!(observations, \"Pain-$(pain_level)___BMI-$(bmi_level)\")\n",
    "end\n",
    "push!(observations, \"Dummy\")\n",
    "# observations = [ \"Pain-High___BMI-High\", \"Pain-High___BMI-Med\", \"Pain-High___BMI-Low\", \n",
    "#     \"Pain-Med___BMI-High\", \"Pain-Med___BMI-Med\", \"Pain-Med___BMI-Low\",\n",
    "#     \"Pain-Low___BMI-High\", \"Pain-Low___BMI-Med\", \"Pain-Low___BMI-Low\"]\n",
    "\n",
    "num_observations = length(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"Pain-Low___BMI-Low\"\n",
       " \"Pain-Low___BMI-Med\"\n",
       " \"Pain-Low___BMI-High\"\n",
       " \"Pain-Med___BMI-Low\"\n",
       " \"Pain-Med___BMI-Med\"\n",
       " \"Pain-Med___BMI-High\"\n",
       " \"Pain-High___BMI-Low\"\n",
       " \"Pain-High___BMI-Med\"\n",
       " \"Pain-High___BMI-High\"\n",
       " \"Dummy\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display observations\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State transition probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional starting state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Optional starting state \n",
    "# Since first decision epoch is at F10, the starting probabilities (at full budget and start of horizon) should be the following:\n",
    "ΔNTCP_start_dist = [0.5, 0.13, 0.08, 0.11, 0.04, 0.04, 0.0, 0.02, 0.0, 0.0, 0.02, 0.02, 0.04]\n",
    "# This corresponds to the states at the beginning\n",
    "start_dist =[ΔNTCP_start_dist; zeros(num_states - num_ΔNTCP_states)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probabilities are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid probabilities\n"
     ]
    }
   ],
   "source": [
    "sanity_check_prob(ΔNTCP_start_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ΔNTCP transitions under Replanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Matrix{Float64}}:\n",
       " [0.88 0.0 … 0.0 0.0; 0.88 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.88 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.88 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.88 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Toxicity state transition probabilities\n",
    "## Replan, F0 to F10\n",
    "# THIS IS NOT A DECISION EPOCH\n",
    "# Replan isn't an option at time 0. The trans. prob. of Continue at F0 correspond to the starting distribution\n",
    "# of the POMDP (see starting state dist. above)\n",
    "\n",
    "## Replan, F10 to F15, table B.3\n",
    "T_ΔNTCP_F10toF15_R = [\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0\n",
    "]\n",
    "\n",
    "## Replan, F15 to F20, table B.4\n",
    "T_ΔNTCP_F15toF20_R = [\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
    "]\n",
    "\n",
    "## Replan, F20 to F25, table B.5\n",
    "T_ΔNTCP_F20toF25_R = [\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
    "]\n",
    "\n",
    "## Replan, F25 to F30, table B.6\n",
    "T_ΔNTCP_F25toF30_R = [\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
    "]\n",
    "\n",
    "# Store them in a vector\n",
    "T_ΔNTCP_all_R = [T_ΔNTCP_F10toF15_R, T_ΔNTCP_F15toF20_R, T_ΔNTCP_F20toF25_R, T_ΔNTCP_F25toF30_R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple stylized ΔNTCP transitions under Replanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Matrix{Float64}}:\n",
       " [0.8 0.2 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.05 0.0; 0.0 0.0 … 0.05 0.05]\n",
       " [0.8 0.2 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.05 0.0; 0.0 0.0 … 0.05 0.05]\n",
       " [0.8 0.2 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.05 0.0; 0.0 0.0 … 0.05 0.05]\n",
       " [0.8 0.2 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.05 0.0; 0.0 0.0 … 0.05 0.05]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Toxicity state transition probabilities\n",
    "## Replan, F0 to F10\n",
    "# THIS IS NOT A DECISION EPOCH\n",
    "# Replan isn't an option at time 0. The trans. prob. of Continue at F0 correspond to the starting distribution\n",
    "# of the POMDP (see starting state dist. above)\n",
    "\n",
    "## Replan, F10 to F15, table B.3\n",
    "T_ΔNTCP_F10toF15_R = [\n",
    "    0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.35 0.35 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.35 0.35 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.35 0.35 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.2 0.3 0.3 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.2 0.3 0.3 0.2 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.25 0.25 0.25 0.15 0.1 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.2 0.25 0.25 0.1 0.1 0.1 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.2 0.25 0.25 0.1 0.1 0.1 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.2 0.25 0.25 0.1 0.1 0.1 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.15 0.2 0.25 0.25 0.05 0.05 0.05 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.15 0.2 0.25 0.25 0.05 0.05 0.05;\n",
    "]\n",
    "# T_ΔNTCP_F10toF15_R = [\n",
    "#     0.5 0.3 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.2 0.5 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0 0.0 0.0;\n",
    "#     0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0 0.0;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0 0.0;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0 0.0;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1 0.0;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.2 0.1;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.5 0.2;\n",
    "#     0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.3 0.5\n",
    "# ]\n",
    "\n",
    "## Replan, F15 to F20, table B.4\n",
    "T_ΔNTCP_F15toF20_R = copy(T_ΔNTCP_F10toF15_R)  # Matrix{Float64}(I(num_ΔNTCP_states))\n",
    "\n",
    "## Replan, F20 to F25, table B.5\n",
    "T_ΔNTCP_F20toF25_R = copy(T_ΔNTCP_F10toF15_R)\n",
    "\n",
    "## Replan, F25 to F30, table B.6\n",
    "T_ΔNTCP_F25toF30_R = copy(T_ΔNTCP_F10toF15_R)\n",
    "\n",
    "# Store them in a vector\n",
    "T_ΔNTCP_all_R = [T_ΔNTCP_F10toF15_R, T_ΔNTCP_F15toF20_R, T_ΔNTCP_F20toF25_R, T_ΔNTCP_F25toF30_R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ΔNTCP transitions under No Replan (i.e., Continuing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continue, F0 to F10, table B.1\n",
    "# THIS IS NOT A DECISION EPOCH\n",
    "# The trans. prob. of Continue at F0 correspond to the starting distribution of the POMDP (see starting state dist. above)\n",
    "## Continue, F10 to F15, table B.2\n",
    "T_ΔNTCP_F10toF15_C = [\n",
    "    0.88 0.0 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.5 0.33 0.17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5\n",
    "]\n",
    "## Continue, F15 to F20, table B.2\n",
    "T_ΔNTCP_F15toF20_C = copy(T_ΔNTCP_F10toF15_C)\n",
    "## Continue, F20 to F25, table B.2\n",
    "T_ΔNTCP_F20toF25_C = copy(T_ΔNTCP_F10toF15_C)\n",
    "## Continue, F25 to F30, table B.2\n",
    "T_ΔNTCP_F25toF30_C = copy(T_ΔNTCP_F10toF15_C)\n",
    "\n",
    "# Store them in a vector\n",
    "T_ΔNTCP_all_C = [T_ΔNTCP_F10toF15_C, T_ΔNTCP_F15toF20_C, T_ΔNTCP_F20toF25_C, T_ΔNTCP_F25toF30_C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple stylized ΔNTCP transitions under No Replan (i.e., Continuing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Matrix{Float64}}:\n",
       " [1.0 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.25 0.0; 0.0 0.0 … 0.5 0.25]\n",
       " [1.0 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.25 0.0; 0.0 0.0 … 0.5 0.25]\n",
       " [1.0 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.25 0.0; 0.0 0.0 … 0.5 0.25]\n",
       " [1.0 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.25 0.0; 0.0 0.0 … 0.5 0.25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Continue, F0 to F10, table B.1\n",
    "# THIS IS NOT A DECISION EPOCH\n",
    "# The trans. prob. of Continue at F0 correspond to the starting distribution of the POMDP (see starting state dist. above)\n",
    "## Continue, F10 to F15, table B.2\n",
    "T_ΔNTCP_F10toF15_C = [\n",
    "    1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25\n",
    "]\n",
    "## Continue, F15 to F20, table B.2\n",
    "T_ΔNTCP_F15toF20_C = copy(T_ΔNTCP_F10toF15_C)  # Matrix{Float64}(I(num_ΔNTCP_states))\n",
    "## Continue, F20 to F25, table B.2\n",
    "T_ΔNTCP_F20toF25_C = copy(T_ΔNTCP_F10toF15_C)\n",
    "## Continue, F25 to F30, table B.2\n",
    "T_ΔNTCP_F25toF30_C = copy(T_ΔNTCP_F10toF15_C)\n",
    "\n",
    "# Store them in a vector\n",
    "T_ΔNTCP_all_C = [T_ΔNTCP_F10toF15_C, T_ΔNTCP_F15toF20_C, T_ΔNTCP_F20toF25_C, T_ΔNTCP_F25toF30_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect into a dictionary for ease of access by Action\n",
    "T_ΔNTCP_all = Dict(\"Replan\" => T_ΔNTCP_all_R, \"Continue\" => T_ΔNTCP_all_C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create master state transition probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Full state transition probabilities\n",
    "# T: <action> : <start-state> : <end-state> %f\n",
    "\n",
    "T = zeros(length(actions), num_states, num_states);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place probability submatrices into the master state transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_ind in eachindex(actions)\n",
    "    # println(\"\\nAction: $(actions[a_ind])\")\n",
    "    for t = 1:horizon\n",
    "        # println(\"Time = $t\")\n",
    "        # b serves as a counter here\n",
    "        if actions[a_ind] == \"Replan\"\n",
    "            for b = 1:num_budget_states-1\n",
    "                row_indices = ((t-1)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t-1)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "                col_indices = ((t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states +1):((t)*num_budget_states*num_ΔNTCP_states +(b+1)*num_ΔNTCP_states)\n",
    "                # println(\"\\trow indices: $row_indices\")\n",
    "                # println(\"\\tcol indices: $col_indices\")\n",
    "                T[a_ind, row_indices, col_indices] = T_ΔNTCP_all[actions[a_ind]][t]\n",
    "            end\n",
    "        elseif actions[a_ind] == \"Continue\"\n",
    "            for b = 1:num_budget_states\n",
    "                row_indices = ((t-1)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t-1)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "                col_indices = ((t)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "                # println(\"\\trow indices: $row_indices\")\n",
    "                # println(\"\\tcol indices: $col_indices\")\n",
    "                T[a_ind, row_indices, col_indices] = T_ΔNTCP_all[actions[a_ind]][t]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add remaining probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Absorbing probabilities\n",
    "# Whenever the budget is 0 and action is Replan, transition to the absorbing state\n",
    "for t = 1:horizon\n",
    "    ind_start = (t-1)*num_ΔNTCP_states*num_budget_states + (num_budget_states-1)*num_ΔNTCP_states + 1\n",
    "    ind_end = (t-1)*num_ΔNTCP_states*num_budget_states + (num_budget_states)*num_ΔNTCP_states\n",
    "    # println(\"Range: $ind_start:$ind_end\")\n",
    "    # println(\"States: $(states[ind_start:ind_end])\")\n",
    "    T[1, ind_start:ind_end, end] .= 1.0  # last state is the absorbing state\n",
    "end\n",
    "\n",
    "## Recursion of the absorbing state\n",
    "T[1,end,end] = 1.0\n",
    "T[2,end,end] = 1.0\n",
    "\n",
    "# ## Identity probabilities at horizon+1 for both actions, since this is not a real decision epoch\n",
    "# range = horizon *num_ΔNTCP_states *num_budget_states +1:(horizon+1) *num_ΔNTCP_states *num_budget_states\n",
    "# T[1,range, range] = Matrix{Int64}(I(num_ΔNTCP_states*num_budget_states))\n",
    "# T[2,range, range] = Matrix{Int64}(I(num_ΔNTCP_states*num_budget_states));\n",
    "\n",
    "## At horizon+1 (not a real decision epoch), transition back to starting state (according to ΔNTCP_start_dist) no matter the action\n",
    "for b = 1:num_budget_states\n",
    "    row_range = (horizon*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):(horizon*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "    for row_ind in row_range\n",
    "        T[1, row_ind, 1:num_ΔNTCP_states] = copy(ΔNTCP_start_dist)\n",
    "        T[2, row_ind, 1:num_ΔNTCP_states] = copy(ΔNTCP_start_dist)\n",
    "    end\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probabilities are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_rows_R: Tuple{Int64, Float64}[]\n",
      "bad_rows_C: Tuple{Int64, Float64}[]\n"
     ]
    }
   ],
   "source": [
    "bad_rows_R = sanity_check_prob(T[1,:,:])\n",
    "bad_rows_C = sanity_check_prob(T[2,:,:])\n",
    "println(\"bad_rows_R: $bad_rows_R\\nbad_rows_C: $bad_rows_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_R = map(row_ind -> sum(T[1,:,:][row_ind,:]) - 1.0, 1:size(T[1,:,:],1))\n",
    "res_C = map(row_ind -> sum(T[2,:,:][row_ind,:]) - 1.0, 1:size(T[2,:,:],1))\n",
    "\n",
    "println(\"Replan\")\n",
    "foreach(row -> if sum(row) != 0.0 println(\"$(sum(row))\") end, res_R)\n",
    "println(\"Continue\")\n",
    "foreach(row -> if sum(row) != 0.0 println(\"$(sum(row))\") end, res_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write state transition matrix to an excel file (with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write state transition matrix to an excel file (with labels)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time- and budget-independent (ΔNTCP, obs) submatrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×9 Matrix{Float64}:\n",
       " 0.166667   0.166667   0.166667   …  0.0833333  0.0416667  0.0416667\n",
       " 0.166667   0.166667   0.166667      0.0833333  0.0416667  0.0416667\n",
       " 0.166667   0.166667   0.166667      0.0833333  0.0416667  0.0416667\n",
       " 0.0769231  0.0769231  0.153846      0.0769231  0.0769231  0.0769231\n",
       " 0.0769231  0.0769231  0.153846      0.0769231  0.0769231  0.0769231\n",
       " 0.0769231  0.0769231  0.0769231  …  0.153846   0.0769231  0.0769231\n",
       " 0.0769231  0.0769231  0.0769231     0.153846   0.0769231  0.0769231\n",
       " 0.0769231  0.0769231  0.0769231     0.153846   0.0769231  0.0769231\n",
       " 0.04       0.08       0.08          0.16       0.16       0.08\n",
       " 0.04       0.08       0.08          0.16       0.16       0.08\n",
       " 0.0416667  0.0416667  0.0833333  …  0.166667   0.166667   0.166667\n",
       " 0.0416667  0.0416667  0.0833333     0.166667   0.166667   0.166667\n",
       " 0.0416667  0.0416667  0.0833333     0.166667   0.166667   0.166667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Will assume the observation probabilities are independent of budget and time (may consider time dependence)\n",
    "# The worse a ΔNTCP state, the likelier for a worse observation\n",
    "# Will assume observations are ordered from best to worse, with high pain being worse than a high BMI drop\n",
    "\n",
    "# Observation probabilities should be independent of the actions, what matters is the new state\n",
    "# The action influences the state transition probabilities\n",
    "\n",
    "# The following values are made up for testing\n",
    "O_sub = [\n",
    "    1/6 1/6 1/6 1/6 1/12 1/12 1/12 1/24 1/24;\n",
    "    1/6 1/6 1/6 1/6 1/12 1/12 1/12 1/24 1/24;\n",
    "    1/6 1/6 1/6 1/6 1/12 1/12 1/12 1/24 1/24;\n",
    "    5/65 5/65 8/52 8/52 8/52 8/52 5/65 5/65 5/65;\n",
    "    5/65 5/65 8/52 8/52 8/52 8/52 5/65 5/65 5/65;\n",
    "    5/65 5/65 5/65 8/52 8/52 8/52 8/52 5/65 5/65;\n",
    "    5/65 5/65 5/65 8/52 8/52 8/52 8/52 5/65 5/65;\n",
    "    5/65 5/65 5/65 8/52 8/52 8/52 8/52 5/65 5/65;\n",
    "    1/25 2/25 2/25 2/25 4/25 4/25 4/25 4/25 2/25;\n",
    "    1/25 2/25 2/25 2/25 4/25 4/25 4/25 4/25 2/25;\n",
    "    1/24 1/24 1/12 1/12 1/12 1/6 1/6 1/6 1/6;\n",
    "    1/24 1/24 1/12 1/12 1/12 1/6 1/6 1/6 1/6;\n",
    "    1/24 1/24 1/12 1/12 1/12 1/6 1/6 1/6 1/6\n",
    "];\n",
    "# O_sub = zeros(num_ΔNTCP_states, num_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = map(row_ind -> sum(O_sub[row_ind,:]) - 1.0, 1:size(O_sub,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple stylized observation probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×9 Matrix{Float64}:\n",
       " 0.75  0.25  0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " 0.25  0.5   0.25  0.0   0.0   0.0   0.0   0.0   0.0\n",
       " 0.0   0.25  0.5   0.25  0.0   0.0   0.0   0.0   0.0\n",
       " 0.0   0.0   0.25  0.5   0.25  0.0   0.0   0.0   0.0\n",
       " 0.0   0.0   0.0   0.25  0.5   0.25  0.0   0.0   0.0\n",
       " 0.0   0.0   0.0   0.0   0.25  0.5   0.25  0.0   0.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.25  0.5   0.25  0.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.25  0.5   0.25\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.25  0.75\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Will assume the observation probabilities are independent of budget and time (may consider time dependence)\n",
    "# The worse a ΔNTCP state, the likelier for a worse observation\n",
    "# Will assume observations are ordered from best to worse, with high pain being worse than a high BMI drop\n",
    "\n",
    "# Observation probabilities should be independent of the actions, what matters is the new state\n",
    "# The action influences the state transition probabilities\n",
    "\n",
    "# The following values are made up for testing\n",
    "O_sub = [\n",
    "    0.75 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25 0.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.5 0.25;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 0.75;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0;\n",
    "    0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
    "]\n",
    "# O_sub = zeros(num_ΔNTCP_states, num_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probabilities are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple{Int64, Float64}[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_rows = sanity_check_prob(O_sub) # currently has some rounding issues, hopefully will not break the solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create master observation probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Observation probabilities\n",
    "# An observation is observed after the state transitions, hence depends on the new state\n",
    "# O: <action> : <end-state> : <observation> %f\n",
    "\n",
    "O = zeros(length(actions), num_states, num_observations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place probability submatrices into the master observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall it is the observation observed at the end state\n",
    "\n",
    "# First, have all end-states observe the dummy observation\n",
    "for a in eachindex(actions)\n",
    "    O[a, :, end] .= 1.0\n",
    "end\n",
    "\n",
    "## Next, update probabilities for end-states that shouldn't observe the dummy observation\n",
    "## Replan\n",
    "for t = 1:horizon\n",
    "    for b = 1:min(t,horizon-1) # min(t,num_budget_states-1):num_budget_states-1\n",
    "        ind_start = (t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states +1\n",
    "        ind_end = (t)*num_budget_states*num_ΔNTCP_states +(b+1)*num_ΔNTCP_states\n",
    "        O[1,ind_start:ind_end,1:end-1] = O_sub\n",
    "        O[1,ind_start:ind_end,end] .= 0.0  # Remove observing the dummy state\n",
    "\n",
    "        # foreach(i -> println(\"$(states[i])\"), ind_start:ind_end)\n",
    "    end\n",
    "end\n",
    "# println(\"\\n\\n\\n\")\n",
    "## Continue\n",
    "for t = 1:horizon\n",
    "    for b = 1:min(t,horizon) # 1:num_budget_states\n",
    "        ind_start = (t)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1\n",
    "        ind_end = (t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states\n",
    "        O[2,ind_start:ind_end,1:end-1] = O_sub\n",
    "        O[2,ind_start:ind_end,end] .= 0.0  # Remove observing the dummy state\n",
    "\n",
    "        # foreach(i -> println(\"$(states[i])\"), ind_start:ind_end)\n",
    "    end\n",
    "end\n",
    "\n",
    "## Add observation probabilities for (b,t) = (full-budget,1)\n",
    "# These states should observe the dummy observation; ignore the following\n",
    "# O[1,1:num_ΔNTCP_states,1:end-1] = O_sub\n",
    "# O[2,1:num_ΔNTCP_states,1:end-1] = O_sub\n",
    "# O[1,1:num_ΔNTCP_states,end] .= 0.0\n",
    "# O[2,1:num_ΔNTCP_states,end] .= 0.0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probabilities are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rows_R = sanity_check_prob(O[1,:,:])\n",
    "# foreach(tup -> println(\"$(states[tup[1]])\"), bad_rows_R)\n",
    "foreach(tup -> println(\"$(states[tup[1]]) --- $(tup[2])\"), bad_rows_R)\n",
    "\n",
    "bad_rows_C = sanity_check_prob(O[2,:,:])\n",
    "# foreach(tup -> println(\"$(states[tup[1]])\"), bad_rows_C)\n",
    "foreach(tup -> println(\"$(states[tup[1]]) --- $(tup[2])\"), bad_rows_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write observation matrix to an excel file (with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write observation probability matrix to an excel file (with labels)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediate rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×13 Matrix{Int64}:\n",
       "  0  -1  -2  -3  -4  -5  -6  -7  -8  -9  -10  -11  -12\n",
       "  1   0  -1  -2  -3  -4  -5  -6  -7  -8   -9  -10  -11\n",
       "  2   1   0  -1  -2  -3  -4  -5  -6  -7   -8   -9  -10\n",
       "  3   2   1   0  -1  -2  -3  -4  -5  -6   -7   -8   -9\n",
       "  4   3   2   1   0  -1  -2  -3  -4  -5   -6   -7   -8\n",
       "  5   4   3   2   1   0  -1  -2  -3  -4   -5   -6   -7\n",
       "  6   5   4   3   2   1   0  -1  -2  -3   -4   -5   -6\n",
       "  7   6   5   4   3   2   1   0  -1  -2   -3   -4   -5\n",
       "  8   7   6   5   4   3   2   1   0  -1   -2   -3   -4\n",
       "  9   8   7   6   5   4   3   2   1   0   -1   -2   -3\n",
       " 10   9   8   7   6   5   4   3   2   1    0   -1   -2\n",
       " 11  10   9   8   7   6   5   4   3   2    1    0   -1\n",
       " 12  11  10   9   8   7   6   5   4   3    2    1    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# R: <action> : <start-state> : <end-state> : <observation> %f\n",
    "\n",
    "# Store master reward matrices in a dictionary to access by Action\n",
    "R = Dict(\"Replan\" => zeros(num_states, num_states, num_observations), \"Continue\" => zeros(num_states, num_states, num_observations))\n",
    "# Rewards across ΔNTCP states\n",
    "# Have we addresed the rewards from transitioning to a forbidden state? I believe we only need to consider replanning when b=0. \n",
    "# Actions at unreachable states shouldn't matter because they have 0 probability\n",
    "R_NTCP = [-(s_end - s_start) for s_start ∈ ΔNTCP_states, s_end ∈ ΔNTCP_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale R_NTCP by severity of each observation (if we'd like; otherwise, scale by 1 so that we only care about the ΔNTCP state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we are assuming observations are ordered from best to worst, with high pain being worse than a high BMI drop\n",
    "# The observation_intensities are to scale the rewards by the severity of the observation\n",
    "observation_intensities = [i for i = num_observations:-1:1] # we are maximizing so decreasing values\n",
    "# If we prefer rewards be independent of the observation (hence, no scaling necessary)\n",
    "observation_intensities = [1 for _ = 1:num_observations]\n",
    "\n",
    "## Replan\n",
    "for t = 1:horizon\n",
    "    for b = 1:num_budget_states-1\n",
    "        row_indices = ((t-1)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t-1)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "        col_indices = ((t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states +1):((t)*num_budget_states*num_ΔNTCP_states +(b+1)*num_ΔNTCP_states)\n",
    "        # println(\"\\trow indices: $row_indices\")\n",
    "        # println(\"\\tcol indices: $col_indices\")\n",
    "        for obs_ind in eachindex(observation_intensities)\n",
    "            R[\"Replan\"][row_indices, col_indices, obs_ind] = R_NTCP * observation_intensities[obs_ind]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "## Rewards going back to initial states should be 0, so don't need the following bc already are 0\n",
    "# Add rewards going back to initial states (b,t) = (3,1)\n",
    "# for b = 1:num_budget_states-1\n",
    "#     row_indices = ((horizon)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((horizon)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "#     col_indices = 1:num_ΔNTCP_states\n",
    "#     println(\"\\trow indices: $row_indices\")\n",
    "#     println(\"\\tcol indices: $col_indices\")\n",
    "#     for obs_ind in eachindex(observation_intensities)\n",
    "#         R[\"Replan\"][row_indices, col_indices, obs_ind] = R_NTCP * observation_intensities[obs_ind]\n",
    "#     end\n",
    "# end\n",
    "\n",
    "## Continue\n",
    "for t = 1:horizon\n",
    "    for b = 1:num_budget_states\n",
    "        row_indices = ((t-1)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t-1)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "        col_indices = ((t)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((t)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "        # println(\"\\trow indices: $row_indices\")\n",
    "        # println(\"\\tcol indices: $col_indices\")\n",
    "        for obs_ind in eachindex(observation_intensities)\n",
    "            R[\"Continue\"][row_indices, col_indices, obs_ind] = R_NTCP * observation_intensities[obs_ind]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "## Rewards going back to initial states should be 0, so don't need the following bc already are 0\n",
    "# Add rewards going back to initial states (b,t) = (3,1)\n",
    "# for b = 1:num_budget_states\n",
    "#     row_indices = ((horizon)*num_budget_states*num_ΔNTCP_states +(b-1)*num_ΔNTCP_states +1):((horizon)*num_budget_states*num_ΔNTCP_states +(b)*num_ΔNTCP_states)\n",
    "#     col_indices = 1:num_ΔNTCP_states\n",
    "#     # println(\"\\trow indices: $row_indices\")\n",
    "#     # println(\"\\tcol indices: $col_indices\")\n",
    "#     for obs_ind in eachindex(observation_intensities)\n",
    "#         R[\"Continue\"][row_indices, col_indices, obs_ind] = R_NTCP * observation_intensities[obs_ind]\n",
    "#     end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add remaining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 40:52\n",
      "States: [\"0_0_1\", \"1_0_1\", \"2_0_1\", \"3_0_1\", \"4_0_1\", \"5_0_1\", \"6_0_1\", \"7_0_1\", \"8_0_1\", \"9_0_1\", \"10_0_1\", \"11_0_1\", \"12_0_1\"]\n",
      "Range: 92:104\n",
      "States: [\"0_0_2\", \"1_0_2\", \"2_0_2\", \"3_0_2\", \"4_0_2\", \"5_0_2\", \"6_0_2\", \"7_0_2\", \"8_0_2\", \"9_0_2\", \"10_0_2\", \"11_0_2\", \"12_0_2\"]\n",
      "Range: 144:156\n",
      "States: [\"0_0_3\", \"1_0_3\", \"2_0_3\", \"3_0_3\", \"4_0_3\", \"5_0_3\", \"6_0_3\", \"7_0_3\", \"8_0_3\", \"9_0_3\", \"10_0_3\", \"11_0_3\", \"12_0_3\"]\n",
      "Range: 196:208\n",
      "States: [\"0_0_4\", \"1_0_4\", \"2_0_4\", \"3_0_4\", \"4_0_4\", \"5_0_4\", \"6_0_4\", \"7_0_4\", \"8_0_4\", \"9_0_4\", \"10_0_4\", \"11_0_4\", \"12_0_4\"]\n"
     ]
    }
   ],
   "source": [
    "## Whenever the budget is 0 and action is Replan, give a big negative to reward to prevent this\n",
    "for t = 1:horizon\n",
    "    ind_start = (t-1)*num_ΔNTCP_states*num_budget_states + (num_budget_states-1)*num_ΔNTCP_states + 1\n",
    "    ind_end = (t-1)*num_ΔNTCP_states*num_budget_states + (num_budget_states)*num_ΔNTCP_states\n",
    "    println(\"Range: $ind_start:$ind_end\")\n",
    "    println(\"States: $(states[ind_start:ind_end])\")\n",
    "\n",
    "    # Reward is the same for any state and observation transitioning to the absorbing state (end)\n",
    "    R[\"Replan\"][ind_start:ind_end, end,:] .= -100000.0\n",
    "end\n",
    "\n",
    "## The following are commented because the rewards should already be 0. Uncomment if want to set other values\n",
    "\n",
    "## Rewards at the absorbing state (believe value shouldn't matter)\n",
    "# R[\"Replan\"][end,end,:] .= 0.0\n",
    "# R[\"Continue\"][end,end,:] .= 0.0\n",
    "\n",
    "# ## Rewards at horizon + 1 (believe value shouldn't matter)\n",
    "# range = horizon *num_ΔNTCP_states *num_budget_states +1:(horizon+1) *num_ΔNTCP_states *num_budget_states\n",
    "# R[\"Replan\"][range, range,:] .= zeros(num_ΔNTCP_states*num_budget_states, num_ΔNTCP_states*num_budget_states);\n",
    "# R[\"Continue\"][range, range,:] .= zeros(num_ΔNTCP_states*num_budget_states, num_ΔNTCP_states*num_budget_states);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write input file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/raulgarcia/Documents/Rice University/Research/NIH SCH Adaptive Radiation Therapy/Aim 2_POMDP ART/Input Files/tester-obs_indep_rewards_dummy_obs_stylized_prob_chain_corrected_1.POMDP\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filename = \"tester.POMDP\"\n",
    "trial_num = 1\n",
    "filename = \"tester-obs_indep_rewards_dummy_obs_stylized_prob_chain_corrected_$trial_num.POMDP\"\n",
    "# full_path = joinpath(pwd(), \"Input Files\", \"$filename\")\n",
    "full_path = joinpath(pwd(), \"$filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write or append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IOStream(<file /Users/raulgarcia/Documents/Rice University/Research/NIH SCH Adaptive Radiation Therapy/Aim 2_POMDP ART/Input Files/tester-obs_indep_rewards_dummy_obs_stylized_prob_chain_corrected_1.POMDP>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(full_path, \"w\")\n",
    "# f = open(full_path, \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write(f, \"# Describe this instance here\\n\")\n",
    "write(f, \"# Initial attempt at correcting Markov chain (states at t=T+1 go back to t=1, where t=T+1 is not a real decision epoch)\")\n",
    "write(f, \"# More realistic stylized state transition and observation probabilities. With dummy observation for unreachable states\\n\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five line must be these (can be in a different order though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Discount factor\n",
    "write(f, \"discount: $discount\\n\")\n",
    "\n",
    "#### Rewards\n",
    "write(f, \"values: $values\\n\")\n",
    "\n",
    "#### States\n",
    "write(f, \"states: $num_states\\n\")\n",
    "\n",
    "#### Actions\n",
    "write(f, \"actions: \")\n",
    "for act in actions\n",
    "    write(f, \"$act \")\n",
    "end\n",
    "write(f, \"\\n\")\n",
    "\n",
    "#### Observations\n",
    "write(f, \"observations: \")\n",
    "for obs in observations\n",
    "    write(f, \"$obs \")\n",
    "end\n",
    "write(f, \"\\n\\n\")\n",
    "\n",
    "flush(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial five lines and optional starting state, the specifications of transition probabilities, \n",
    "observation probabilities and rewards appear. These specifications may appear in any order and can be intermixed.\n",
    "Any probabilities or rewards not specified in the file are assumed to be zero. \n",
    "You may also specify a particular probability or reward more than once. The definition that appears last in the file \n",
    "is the one that will take affect. This is convenient for specifying exceptions to a more general specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write starting state distribution (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "write(f, \"start: \")\n",
    "foreach(s_prob -> write(f, \"$s_prob \"), start_dist)\n",
    "write(f, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write state transition probabilitities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in eachindex(actions)\n",
    "    write(f, \"T: $(actions[a])\\n\")\n",
    "    for s_start in Base.OneTo(num_states)\n",
    "        for s_end in Base.OneTo(num_states)\n",
    "            write(f, \"$(T[a,s_start,s_end]) \")\n",
    "        end\n",
    "        write(f, \"\\n\")\n",
    "    end\n",
    "    write(f, \"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write observation matrix probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in eachindex(actions)\n",
    "    write(f, \"O: $(actions[a])\\n\")\n",
    "    for s_end in Base.OneTo(num_states)\n",
    "        for o in Base.OneTo(num_observations)\n",
    "            write(f, \"$(O[a,s_end,o]) \")\n",
    "        end\n",
    "        write(f, \"\\n\")\n",
    "    end\n",
    "    write(f, \"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write immediate rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions, s_start in Base.OneTo(num_states)\n",
    "    write(f, \"R: $action : $(s_start-1)\\n\")\n",
    "    for s_end in Base.OneTo(num_states)\n",
    "        for o in Base.OneTo(num_observations)\n",
    "            write(f, \"$(R[action][s_start,s_end,o]) \")\n",
    "        end\n",
    "        write(f, \"\\n\")\n",
    "    end\n",
    "    write(f, \"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute pomdp-solve via command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`\u001b[4m/Users/raulgarcia/Documents/pomdp-solve-5.5/src/pomdp-solve\u001b[24m \u001b[4m-pomdp\u001b[24m \u001b[4mtester-obs_indep_rewards_dummy_obs_stylized_prob_chain_corrected_1.POMDP\u001b[24m \u001b[4m-method\u001b[24m \u001b[4mgrid\u001b[24m \u001b[4m-fg_points\u001b[24m \u001b[4m1000\u001b[24m`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filename = \"tester.POMDP\"\n",
    "trial_num = 1\n",
    "method = \"grid\"\n",
    "fg_points = 1000 # default 10000; max number of belief points to use in finite grid\n",
    "# fg_save = false # default false; save grid points to external file\n",
    "filename = \"tester-obs_indep_rewards_dummy_obs_stylized_prob_chain_corrected_$trial_num.POMDP\"\n",
    "cmd = `/Users/raulgarcia/Documents/pomdp-solve-5.5/src/pomdp-solve -pomdp $filename -method $method -fg_points $fg_points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optional paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_limit = 60  # seconds\n",
    "cmd = `/Users/raulgarcia/Documents/pomdp-solve-5.5/src/pomdp-solve -pomdp $filename -time_limit $time_limit`\n",
    "run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5\n",
    "cmd = `/Users/raulgarcia/Documents/pomdp-solve-5.5/src/pomdp-solve -pomdp $filename -horizon $max_iter`\n",
    "run(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
